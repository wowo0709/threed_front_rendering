{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3DFront dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"/root/desktop/3D-FRONT/3D-FRONT-processed\"\n",
    "dir_names = {\n",
    "    \"prep\": \"bedrooms_without_lamps\", \n",
    "    \"vert\": \"bedrooms_without_lamps_full_labels_vertices\", \n",
    "    \"norm\": \"bedrooms_without_lamps_full/labels\", \n",
    "    \"cam\": \"bedrooms_without_lamps_full_labels\", \n",
    "    \"cam_fixed_dist\": \"bedrooms_without_lamps_full_labels_fixed_dist\",\n",
    "    \"raw\": \"bedrooms_without_lamps_full_raw/raw_256\",\n",
    "    \"raw_depth\": \"bedrooms_without_lamps_full_raw/raw_256_depth\",\n",
    "    \"raw_depth_normal\": \"bedrooms_without_lamps_full_raw/raw_256_depth_normal\",\n",
    "    \"raw_depth_normal_trans\": \"bedrooms_without_lamps_full_raw/raw_256_depth_normal_trans\",\n",
    "    \"img\": \"bedrooms_without_lamps_full_images/images_256\",\n",
    "    \"img_depth\": \"bedrooms_without_lamps_full_images/images_256_depth\",\n",
    "    \"img_depth_normal\": \"bedrooms_without_lamps_full_images/images_256_depth_normal_noflip_vmax5\",\n",
    "    \"img_depth_normal_trans\": \"bedrooms_without_lamps_full_images/images_256_depth_normal_trans\",\n",
    "    \"img_fixed_dist\": \"bedrooms_without_lamps_full_images_fixed_dist/images_256\", \n",
    "    \"depth_fixed_dist\": \"bedrooms_without_lamps_full_images_fixed_dist/depths_256\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_nums = {\n",
    "    \"prep\": 0, \n",
    "    \"vert\": 0, \n",
    "    \"norm\": 0, \n",
    "    \"cam\": 0, \n",
    "    \"cam_fixed_dist\": 0,\n",
    "    \"raw\": 0,\n",
    "    \"raw_depth\": 0,\n",
    "    \"raw_depth_normal\": 0,\n",
    "    \"raw_depth_normal_trans\": 0,\n",
    "    \"img\": 0,\n",
    "    \"img_depth\": 0,\n",
    "    \"img_depth_normal\": 0,\n",
    "    \"img_depth_normal_trans\": 0,\n",
    "    \"img_fixed_dist\": 0, \n",
    "    \"depth_fixed_dist\": 0,\n",
    "}\n",
    "\n",
    "for id, dir_name in dir_names.items():\n",
    "    data_nums[id] = len(os.listdir(os.path.join(DATA_PATH, dir_name)))\n",
    "    print(f\"ID: {id}    # of data: {data_nums[id]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "EX_SCENE_ID = \"00110bde-f580-40be-b8bb-88715b338a2a_Bedroom-43072\"\n",
    "data_ex_paths = {\n",
    "    \"prep\": 0, \n",
    "    \"vert\": 0, \n",
    "    \"norm\": 0, \n",
    "    \"cam\": 0, \n",
    "    \"raw\": 0,\n",
    "    \"raw_depth\": 0,\n",
    "    \"img\": 0,\n",
    "    \"img_depth\": 0,\n",
    "}\n",
    "\n",
    "for id, dir_name in dir_names.items():\n",
    "    data_path = os.path.join(DATA_PATH, dir_name)\n",
    "    ex_scene_path = os.path.join(data_path, EX_SCENE_ID)\n",
    "    data_ex_paths[id] = ex_scene_path\n",
    "    print(\"[ID]\", id)\n",
    "    print(os.listdir(data_ex_paths[id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_examples = {\n",
    "    \"vert\": 0, \n",
    "    \"norm\": 0, \n",
    "    \"cam\": 0\n",
    "}\n",
    "\n",
    "for id, data_ex_path in data_ex_paths.items():\n",
    "    if id in data_examples.keys():\n",
    "        data_examples[id] = np.load(os.path.join(data_ex_path, \"boxes.npz\"), allow_pickle=True)\n",
    "        print(\"[ID]\", id)\n",
    "        for k in data_examples[id].keys():\n",
    "            print(\"-KEY:\", k)\n",
    "            print(\"-VALUE\", \"\\n\", data_examples[id][k])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Check hdf5 file\n",
    "idx = 0\n",
    "hdf_ex_path = os.path.join(DATA_PATH, dir_names[\"raw_depth_normal\"], EX_SCENE_ID, f\"{idx}.hdf5\")\n",
    "\n",
    "with h5py.File(hdf_ex_path, \"r\") as hdf:\n",
    "    # List all groups\n",
    "    print(\"Keys:\", list(hdf.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EX_SCENE_ID = \"00110bde-f580-40be-b8bb-88715b338a2a_Bedroom-43072\" # \"bbed00a6-d2e0-4f55-a5cc-09db9e3dd6cd_SecondBedroom-45688\"\n",
    "\n",
    "# Check rendered image\n",
    "idx = 0\n",
    "img_types = [\"colors\", \"normals\", \"depth\"]\n",
    "img_ex_paths = {k: 0 for k in img_types}\n",
    "for img_type in img_types:\n",
    "    img_ex_paths[img_type] = os.path.join(DATA_PATH, dir_names[\"img_depth_normal\"], EX_SCENE_ID, str(idx).zfill(4) + f\"_{img_type}\" + \".png\")\n",
    "\n",
    "img_ex_colors = Image.open(img_ex_paths[\"colors\"])\n",
    "img_ex_colors_arr = np.array(img_ex_colors)\n",
    "\n",
    "print(\"Array shape:\", img_ex_colors_arr.shape)\n",
    "print(\"Data type:\", img_ex_colors_arr.dtype)\n",
    "rgb_ex_arr = img_ex_colors_arr[:, :, :3]\n",
    "alpha_ex_arr = img_ex_colors_arr[:, :, 3:]\n",
    "\n",
    "print(np.sum(alpha_ex_arr != 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EX_SCENE_ID = \"00110bde-f580-40be-b8bb-88715b338a2a_Bedroom-43072\" # \"bbed00a6-d2e0-4f55-a5cc-09db9e3dd6cd_SecondBedroom-45688\"\n",
    "\n",
    "# Check rendered image\n",
    "idx = 0\n",
    "img_types = [\"colors\", \"depth\"]\n",
    "img_ex_paths = {k: 0 for k in img_types}\n",
    "for img_type in img_types:\n",
    "    if img_type == \"colors\":\n",
    "        img_ex_paths[img_type] = os.path.join(DATA_PATH, dir_names[\"img_depth_normal\"], EX_SCENE_ID, str(idx).zfill(4) + f\"_{img_type}\" + \".png\")\n",
    "    elif img_type == \"depth\":\n",
    "        img_ex_paths[img_type] = os.path.join(DATA_PATH, dir_names[\"img_depth_normal\"], EX_SCENE_ID, str(idx).zfill(4) + f\"_{img_type}\" + \".png\")\n",
    "\n",
    "img_ex_colors = Image.open(img_ex_paths[\"colors\"])\n",
    "img_ex_colors_arr = np.array(img_ex_colors)\n",
    "\n",
    "print(\"Array shape:\", img_ex_colors_arr.shape)\n",
    "print(\"Data type:\", img_ex_colors_arr.dtype)\n",
    "rgb_ex_arr = img_ex_colors_arr[:, :, :3]\n",
    "alpha_ex_arr = img_ex_colors_arr[:, :, 3:]\n",
    "\n",
    "print(np.sum(alpha_ex_arr != 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plt.imshow(rgb_ex_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "idx = 2\n",
    "img_types = [\"colors\", \"depth\"]\n",
    "img_ex_paths = {k: 0 for k in img_types}\n",
    "for img_type in img_types:\n",
    "    if img_type == \"colors\":\n",
    "        img_ex_paths[img_type] = os.path.join(DATA_PATH, dir_names[\"img_depth_normal\"], EX_SCENE_ID, str(idx).zfill(4) + f\"_{img_type}\" + \".png\")\n",
    "    elif img_type == \"depth\":\n",
    "        img_ex_paths[img_type] = os.path.join(DATA_PATH, dir_names[\"img_depth_normal\"], EX_SCENE_ID, str(idx).zfill(4) + f\"_{img_type}\" + \".png\")\n",
    "\n",
    "img_ex_colors = Image.open(img_ex_paths[\"colors\"])\n",
    "img_ex_colors_arr = np.array(img_ex_colors)\n",
    "\n",
    "print(\"Array shape:\", img_ex_colors_arr.shape)\n",
    "print(\"Data type:\", img_ex_colors_arr.dtype)\n",
    "rgb_ex_arr = img_ex_colors_arr[:, :, :3]\n",
    "alpha_ex_arr = img_ex_colors_arr[:, :, 3:]\n",
    "\n",
    "print(np.sum(alpha_ex_arr != 255))\n",
    "plt.imshow(rgb_ex_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "alpha_mask = alpha_ex_arr.astype(np.float32) / 255.0\n",
    "plt.imshow(alpha_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EX_SCENE_ID = \"00110bde-f580-40be-b8bb-88715b338a2a_Bedroom-43072\" # \"bbed00a6-d2e0-4f55-a5cc-09db9e3dd6cd_SecondBedroom-45688\"\n",
    "\n",
    "# Check rendered image\n",
    "idx = 2\n",
    "img_types = [\"colors\", \"normals\", \"depth\"]\n",
    "img_ex_paths = {k: 0 for k in img_types}\n",
    "for img_type in img_types:\n",
    "    if img_type == \"colors\":\n",
    "        img_ex_paths[img_type] = os.path.join(DATA_PATH, dir_names[\"img_depth_normal\"], EX_SCENE_ID, str(idx).zfill(4) + f\"_{img_type}\" + \".png\")\n",
    "    elif img_type == \"depth\":\n",
    "        img_ex_paths[img_type] = os.path.join(DATA_PATH, dir_names[\"img_depth_normal\"], EX_SCENE_ID, str(idx).zfill(4) + f\"_{img_type}\" + \".png\")\n",
    "\n",
    "# img_ex_normals = Image.open(img_ex_paths[\"normals\"])\n",
    "# img_ex_normals_arr = np.array(img_ex_normals)\n",
    "img_ex_depth = Image.open(img_ex_paths[\"depth\"])\n",
    "img_ex_depth_arr = np.array(img_ex_depth)\n",
    "\n",
    "# print(\"[Array shape]\", \"Normals:\", img_ex_normals_arr.shape, \"Depth:\", img_ex_depth_arr.shape)\n",
    "# print(\"[Data type]\", img_ex_normals_arr.dtype, img_ex_depth_arr.dtype)\n",
    "\n",
    "# normal_ex_arr = img_ex_normals_arr[:, :, :3]\n",
    "# alpha_ex_arr = img_ex_normals_arr[:, :, 3:]\n",
    "# print(\"normal image:\", normal_ex_arr.shape)\n",
    "\n",
    "depth_ex_arr = img_ex_depth_arr[:, :, 0]\n",
    "print(np.all(img_ex_depth_arr[:, :, 0] == img_ex_depth_arr[:, :, 1]))\n",
    "print(\"depth image:\", depth_ex_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EX_SCENE_ID = \"00110bde-f580-40be-b8bb-88715b338a2a_Bedroom-43072\" # \"bbed00a6-d2e0-4f55-a5cc-09db9e3dd6cd_SecondBedroom-45688\"\n",
    "\n",
    "img_ex_depth = Image.open(img_ex_paths[\"depth\"])\n",
    "img_ex_depth_arr = np.array(img_ex_depth)\n",
    "\n",
    "print(\"[Array shape]\", \"Depth:\", img_ex_depth_arr.shape)\n",
    "print(\"[Data type]\", img_ex_depth_arr.dtype)\n",
    "\n",
    "depth_ex_arr = img_ex_depth_arr[:, :, 0]\n",
    "print(np.all(img_ex_depth_arr[:, :, 0] == img_ex_depth_arr[:, :, 1]))\n",
    "print(\"depth image:\", depth_ex_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# plt.imshow(normal_ex_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Anti-aliased misses thin structures\n",
    "print(np.max(depth_ex_arr), np.min(depth_ex_arr))\n",
    "print(depth_ex_arr[150][0])\n",
    "plt.imshow(depth_ex_arr, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "depth_ex_arr_clip = np.clip(depth_ex_arr, 0, 150)\n",
    "plt.imshow(depth_ex_arr_clip, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# (0, 65535) value depth image\n",
    "depth_ex_arr_scaled = (depth_ex_arr.astype(np.uint32) * 256).astype(np.uint32)\n",
    "print(np.max(depth_ex_arr_scaled))\n",
    "plt.imshow(depth_ex_arr_scaled, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "depth_ex_arr_norm = (depth_ex_arr - np.min(depth_ex_arr)) / (np.max(depth_ex_arr) - np.min(depth_ex_arr))\n",
    "plt.imshow(depth_ex_arr_norm, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "depth_ex_arr_log = np.log1p(depth_ex_arr)\n",
    "depth_ex_arr_norm = (depth_ex_arr_log - np.min(depth_ex_arr_log)) / (np.max(depth_ex_arr_log) - np.min(depth_ex_arr_log))\n",
    "plt.imshow(depth_ex_arr_norm, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EX_SCENE_ID = \"00110bde-f580-40be-b8bb-88715b338a2a_Bedroom-43072\" # \"bbed00a6-d2e0-4f55-a5cc-09db9e3dd6cd_SecondBedroom-45688\"\n",
    "\n",
    "# Check rendered image\n",
    "idx = 0\n",
    "img_types = [\"colors\", \"normals\", \"depth\"]\n",
    "img_ex_paths = {k: 0 for k in img_types}\n",
    "for img_type in img_types:\n",
    "    img_ex_paths[img_type] = os.path.join(DATA_PATH, dir_names[\"img_depth_normal_trans\"], EX_SCENE_ID, str(idx).zfill(4) + f\"_{img_type}\" + \".png\")\n",
    "\n",
    "img_ex_colors = Image.open(img_ex_paths[\"colors\"])\n",
    "img_ex_colors_arr = np.array(img_ex_colors)\n",
    "\n",
    "print(\"Array shape:\", img_ex_colors_arr.shape)\n",
    "print(\"Data type:\", img_ex_colors_arr.dtype)\n",
    "rgb_ex_arr = img_ex_colors_arr[:, :, :3]\n",
    "alpha_ex_arr = img_ex_colors_arr[:, :, 3:]\n",
    "\n",
    "print(np.sum(alpha_ex_arr != 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "alpha_mask = alpha_ex_arr.astype(np.float32) / 255.0\n",
    "plt.imshow(alpha_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Check depth\n",
    "import h5py\n",
    "\n",
    "# Check hdf5 file\n",
    "idx = 0\n",
    "hdf_ex_path = os.path.join(DATA_PATH, dir_names[\"raw_depth\"], EX_SCENE_ID, f\"{idx}.hdf5\")\n",
    "\n",
    "with h5py.File(hdf_ex_path, \"r\") as hdf:\n",
    "    # List all groups\n",
    "    print(\"Keys:\", list(hdf.keys()))\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EX_SCENE_ID = \"00110bde-f580-40be-b8bb-88715b338a2a_Bedroom-43072\" # \"bbed00a6-d2e0-4f55-a5cc-09db9e3dd6cd_SecondBedroom-45688\"\n",
    "\n",
    "# Check rendered image\n",
    "idx = 1\n",
    "img_ex_path = os.path.join(DATA_PATH, dir_names[\"img_fixed_dist\"], EX_SCENE_ID, str(idx).zfill(4) + \".png\")\n",
    "\n",
    "img_ex = Image.open(img_ex_path)\n",
    "img_ex_arr = np.array(img_ex)\n",
    "\n",
    "print(\"Array shape:\", img_ex_arr.shape)\n",
    "print(\"Data type:\", img_ex_arr.dtype)\n",
    "rgb_ex_arr = img_ex_arr[:, :, :3]\n",
    "alpha_ex_arr = img_ex_arr[:, :, 3:]\n",
    "\n",
    "print(np.sum(alpha_ex_arr != 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plt.imshow(rgb_ex_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "alpha_mask = alpha_ex_arr.astype(np.float32) / 255.0\n",
    "print(alpha_mask[0][0])\n",
    "plt.imshow(alpha_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "EX_SCENE_ID = \"00110bde-f580-40be-b8bb-88715b338a2a_Bedroom-43072\"\n",
    "render_path = os.path.join(DATA_PATH, dir_names[\"img_depth_normal\"], EX_SCENE_ID)\n",
    "depth_path = os.path.join(DATA_PATH, dir_names[\"img_depth_normal\"], EX_SCENE_ID)\n",
    "camera_path = os.path.join(DATA_PATH, dir_names[\"cam\"], EX_SCENE_ID)\n",
    "\n",
    "print(\"[Render path]\")\n",
    "for filename in os.listdir(render_path):\n",
    "    print(filename)\n",
    "print(\"[Depth path]\")\n",
    "for filename in os.listdir(depth_path):\n",
    "    print(filename)\n",
    "print(\"[Camera path]\")\n",
    "for filename in os.listdir(camera_path):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "camera = np.load(os.path.join(camera_path, \"boxes.npz\"), allow_pickle=True)\n",
    "for k in camera.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(camera[\"floor_plan_centroid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "forward_vec = camera[\"target_coords\"] - camera[\"camera_coords\"]\n",
    "print(\"[Camera coords]\")\n",
    "print(camera[\"camera_coords\"])\n",
    "print(\"[Target coords]\")\n",
    "print(camera[\"target_coords\"])\n",
    "print(\"[Forward vector]\")\n",
    "print(forward_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# !pip install mathutils\n",
    "# !pip install https://github.com/majimboo/py-mathutils/archive/2.78a.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "\n",
    "def rotation_from_forward_vec(forward_vector: Union[np.ndarray, list], up_axis: str = 'Y',\n",
    "                              inplane_rot: Optional[float] = None) -> np.ndarray:\n",
    "    \"\"\" Returns a camera rotation matrix for the given forward vector and up axis using NumPy\n",
    "\n",
    "    :param forward_vec: The forward vector which specifies the direction the camera should look.\n",
    "    :param up_axis: The up axis, usually Y.\n",
    "    :param inplane_rot: The in-plane rotation in radians. If None is given, the in-plane rotation is determined only\n",
    "                        based on the up vector.\n",
    "    :return: The corresponding rotation matrix.\n",
    "    \"\"\"\n",
    "    # Normalize the forward vector\n",
    "    forward_vector = np.array(forward_vector, dtype=np.float64)\n",
    "    forward_vector_norm = forward_vector / np.linalg.norm(forward_vector, axis=1, keepdims=True)\n",
    "\n",
    "    # forward_vec = forward_vec / np.linalg.norm(forward_vec)\n",
    "\n",
    "    # Define the up vector\n",
    "    if up_axis.upper() == 'Y':\n",
    "        up_vec = np.array([0.0, 1.0, 0.0])\n",
    "    elif up_axis.upper() == 'Z':\n",
    "        up_vec = np.array([0.0, 0.0, 1.0])\n",
    "    elif up_axis.upper() == 'X':\n",
    "        up_vec = np.array([1.0, 0.0, 0.0])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid up_axis. Choose from 'X', 'Y', or 'Z'.\")\n",
    "\n",
    "    # Handle edge cases where forward_vec and up_vec are collinear\n",
    "    # _dot = np.dot(forward_vec, up_vec)\n",
    "    # _abs = np.abs(_dot)\n",
    "    # _close = np.isclose(_abs, 1.0)\n",
    "    # print(_close)\n",
    "    # _any = np.any(_close)\n",
    "    # print(_any)\n",
    "    # print(\"======================\")\n",
    "    # if np.allclose(np.abs(np.dot(forward_vec, up_vec)), 1.0):\n",
    "    #     up_vec = np.array([1.0, 0.0, 0.0]) if up_axis.upper() != 'X' else np.array([0.0, 1.0, 0.0])\n",
    "\n",
    "    # Compute the right vector (cross product of forward and up)\n",
    "    right_vec = np.cross(up_vec, forward_vector_norm)\n",
    "    right_vec /= np.linalg.norm(right_vec)\n",
    "\n",
    "    # Recompute the true up vector (orthogonal to forward and right)\n",
    "    up_vec = np.cross(forward_vector_norm, right_vec)\n",
    "\n",
    "    # Construct the rotation matrix (columns represent right, up, forward)\n",
    "    rotation_matrix = np.stack((right_vec, up_vec, -forward_vector_norm), axis=-1)\n",
    "\n",
    "    # Apply in-plane rotation if specified\n",
    "    if inplane_rot is not None:\n",
    "        inplane_rotation = np.array([\n",
    "            [np.cos(inplane_rot), -np.sin(inplane_rot), 0],\n",
    "            [np.sin(inplane_rot),  np.cos(inplane_rot), 0],\n",
    "            [0,                   0,                   1]\n",
    "        ])\n",
    "        rotation_matrix = rotation_matrix @ inplane_rotation\n",
    "\n",
    "    return rotation_matrix\n",
    "\n",
    "\n",
    "forward_vec = camera[\"target_coords\"] - camera[\"camera_coords\"]\n",
    "\n",
    "# print(\"forward_vec\", forward_vec)\n",
    "print(\"[Before] foward_vec:\", - forward_vec / np.linalg.norm(forward_vec, axis=1, keepdims=True))\n",
    "# print(\"forward_vec\", forward_vec)\n",
    "rotation_matrix = rotation_from_forward_vec(forward_vec)\n",
    "forward_vec = rotation_matrix[:, :, 2]\n",
    "print(\"[After] foward_vec:\", forward_vec)\n",
    "# print(rotation_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import zipfile\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, BinaryIO, Union, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# # From ATISS CameraUtility.py\n",
    "# def rotation_from_forward_vec(forward, up=np.array([0, 1, 0])) -> np.ndarray:\n",
    "#     # Normalize the forward vector\n",
    "#     forward = forward / np.linalg.norm(forward)\n",
    "\n",
    "#     # Ensure the up vector is not collinear with the forward vector\n",
    "#     if np.allclose(forward, up) or np.allclose(forward, -up):\n",
    "#         up = np.array([1, 0, 0]) # Use an alternative default up vector\n",
    "\n",
    "#     # Compute the right vector\n",
    "#     right = np.cross(forward, up)\n",
    "#     right = right / np.linalg.norm(right)\n",
    "\n",
    "#     # Recompute the up vector to ensure orthogonality\n",
    "#     up = np.cross(right, forward)\n",
    "\n",
    "#     return right, up, forward\n",
    "#     # # Assemble the rotation matrix\n",
    "#     # rotation_matrix = np.array([right, up, forward])\n",
    "#     # return rotation_matrix.T\n",
    "\n",
    "def rotation_from_forward_vec(forward_vec: Union[np.ndarray, list], up_axis: str = 'Y',\n",
    "                              inplane_rot: Optional[float] = None) -> np.ndarray:\n",
    "    \"\"\" Returns a camera rotation matrix for the given forward vector and up axis using NumPy\n",
    "\n",
    "    :param forward_vec: The forward vector which specifies the direction the camera should look.\n",
    "    :param up_axis: The up axis, usually Y.\n",
    "    :param inplane_rot: The in-plane rotation in radians. If None is given, the in-plane rotation is determined only\n",
    "                        based on the up vector.\n",
    "    :return: The corresponding rotation matrix.\n",
    "    \"\"\"\n",
    "    # Normalize the forward vector\n",
    "    forward_vector = np.array(forward_vec, dtype=np.float64)\n",
    "    forward_vector_norm = forward_vector / np.linalg.norm(forward_vector, axis=1, keepdims=True)\n",
    "\n",
    "    # forward_vec = forward_vec / np.linalg.norm(forward_vec)\n",
    "\n",
    "    # Define the up vector\n",
    "    if up_axis.upper() == 'Y':\n",
    "        up_vec = np.array([0.0, 1.0, 0.0])\n",
    "    elif up_axis.upper() == 'Z':\n",
    "        up_vec = np.array([0.0, 0.0, 1.0])\n",
    "    elif up_axis.upper() == 'X':\n",
    "        up_vec = np.array([1.0, 0.0, 0.0])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid up_axis. Choose from 'X', 'Y', or 'Z'.\")\n",
    "\n",
    "    # Handle edge cases where forward_vec and up_vec are collinear\n",
    "    # _dot = np.dot(forward_vec, up_vec)\n",
    "    # _abs = np.abs(_dot)\n",
    "    # _close = np.isclose(_abs, 1.0)\n",
    "    # print(_close)\n",
    "    # _any = np.any(_close)\n",
    "    # print(_any)\n",
    "    # print(\"======================\")\n",
    "    # if np.allclose(np.abs(np.dot(forward_vec, up_vec)), 1.0):\n",
    "    #     up_vec = np.array([1.0, 0.0, 0.0]) if up_axis.upper() != 'X' else np.array([0.0, 1.0, 0.0])\n",
    "\n",
    "    # Compute the right vector (cross product of forward and up)\n",
    "    # right_vec = np.cross(up_vec, forward_vector_norm) # left-hand\n",
    "    right_vec = np.cross(forward_vector_norm, up_vec)   # right-hand\n",
    "    right_vec /= np.linalg.norm(right_vec, axis=1, keepdims=True)\n",
    "\n",
    "    # Recompute the true up vector (orthogonal to forward and right)\n",
    "    # up_vec = np.cross(forward_vector_norm, right_vec)\n",
    "    up_vec = np.cross(right_vec, forward_vector_norm)\n",
    "    up_vec /= np.linalg.norm(up_vec, axis=1, keepdims=True)\n",
    "\n",
    "    # Construct the rotation matrix (columns represent right, up, forward)\n",
    "    rotation_matrix = np.stack((right_vec, up_vec, -forward_vector_norm), axis=-1)\n",
    "    # rotation_matrix = np.stack((right_vec, up_vec, -forward_vector_norm), axis=1)\n",
    "\n",
    "    # Apply in-plane rotation if specified\n",
    "    if inplane_rot is not None:\n",
    "        inplane_rotation = np.array([\n",
    "            [np.cos(inplane_rot), -np.sin(inplane_rot), 0],\n",
    "            [np.sin(inplane_rot),  np.cos(inplane_rot), 0],\n",
    "            [0,                   0,                   1]\n",
    "        ])\n",
    "        rotation_matrix = rotation_matrix @ inplane_rotation\n",
    "\n",
    "    return rotation_matrix\n",
    "\n",
    "def build_transformation_matrix(camera_coords, rotation_matrix):\n",
    "    camera_coords = np.asarray(camera_coords, dtype=np.float64)\n",
    "    rotation_matrix = np.asarray(rotation_matrix, dtype=np.float64)\n",
    "\n",
    "    assert camera_coords.shape == (3,), \"camera_coords must be a 3-element vector\"\n",
    "    assert rotation_matrix.shape == (3, 3), \"rotation_matrix must be a 3x3 matrix\"\n",
    "\n",
    "    transformation_matrix = np.eye(4)\n",
    "    transformation_matrix[:3, :3] = rotation_matrix\n",
    "    transformation_matrix[:3, 3] = camera_coords\n",
    "\n",
    "    return transformation_matrix\n",
    "\n",
    "def local_to_global(local_points, transformation_matrices):\n",
    "    \"\"\"\n",
    "    Converts local coordinates to global coordinates using transformation matrices.\n",
    "    \n",
    "    Supports input shapes:\n",
    "    - (3,) -> Single 3D point\n",
    "    - (N, 3) -> Multiple 3D points\n",
    "    - (B, N, 3) -> Batch of multiple 3D point clouds\n",
    "    \n",
    "    :param local_points: Local coordinates (shape: (3,), (N, 3), or (B, N, 3)).\n",
    "    :param transformation_matrices: Transformation matrices (shape: (4, 4), (B, 4, 4)).\n",
    "    :return: Global coordinates in the same shape as `local_points`.\n",
    "    \"\"\"\n",
    "    local_points = np.asarray(local_points, dtype=np.float64)\n",
    "    transformation_matrices = np.asarray(transformation_matrices, dtype=np.float64)\n",
    "    \n",
    "    # Single point (3,)\n",
    "    if local_points.ndim == 1 and local_points.shape == (3,):\n",
    "        if transformation_matrices.shape != (4, 4):\n",
    "            raise ValueError(\"For input shape (3,), transformation matrix must be (4, 4).\")\n",
    "        # Convert to homogeneous coordinates\n",
    "        local_point_homogeneous = np.append(local_points, 1.0)  # Shape: (4,)\n",
    "        global_point_homogeneous = transformation_matrices @ local_point_homogeneous  # Shape: (4,)\n",
    "        return global_point_homogeneous[:3]  # Extract (x, y, z)\n",
    "    \n",
    "    # Multiple points (N, 3)\n",
    "    elif local_points.ndim == 2 and local_points.shape[1] == 3:\n",
    "        if transformation_matrices.shape != (4, 4):\n",
    "            raise ValueError(\"For input shape (N, 3), transformation matrix must be (4, 4).\")\n",
    "        # Convert to homogeneous coordinates\n",
    "        local_points_homogeneous = np.hstack((local_points, np.ones((local_points.shape[0], 1))))  # Shape: (N, 4)\n",
    "        global_points_homogeneous = local_points_homogeneous @ transformation_matrices.T  # Shape: (N, 4)\n",
    "        return global_points_homogeneous[:, :3]  # Extract (x, y, z)\n",
    "    \n",
    "    # Batch of point clouds (B, N, 3)\n",
    "    elif local_points.ndim == 3 and local_points.shape[2] == 3:\n",
    "        if transformation_matrices.ndim != 3 or transformation_matrices.shape[1:] != (4, 4):\n",
    "            raise ValueError(\"For input shape (B, N, 3), transformation matrices must be (B, 4, 4).\")\n",
    "        B, N, _ = local_points.shape\n",
    "        # Convert to homogeneous coordinates\n",
    "        local_points_homogeneous = np.concatenate((local_points, np.ones((B, N, 1))), axis=-1)  # Shape: (B, N, 4)\n",
    "        global_points_homogeneous = np.einsum('bij,bkj->bki', transformation_matrices, local_points_homogeneous)  # Shape: (B, N, 4)\n",
    "        return global_points_homogeneous[:, :, :3]  # Extract (x, y, z)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid input shape. Expected (3,), (N, 3), or (B, N, 3).\")\n",
    "\n",
    "@dataclass\n",
    "class Camera(ABC):\n",
    "    \"\"\"\n",
    "    An object describing how a camera corresponds to pixels in an image.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def image_coords(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :return: ([self.height, self.width, 2]).reshape(self.height * self.width, 2) image coordinates\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def camera_rays(self, coords: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        For every (x, y) coordinate in a rendered image, compute the ray of the\n",
    "        corresponding pixel.\n",
    "\n",
    "        :param coords: an [N x 2] integer array of 2D image coordinates.\n",
    "        :return: an [N x 2 x 3] array of [2 x 3] (origin, direction) tuples.\n",
    "                 The direction should always be unit length.\n",
    "        \"\"\"\n",
    "\n",
    "    def depth_directions(self, coords: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        For every (x, y) coordinate in a rendered image, get the direction that\n",
    "        corresponds to \"depth\" in an RGBD rendering.\n",
    "\n",
    "        This may raise an exception if there is no \"D\" channel in the\n",
    "        corresponding ViewData.\n",
    "\n",
    "        :param coords: an [N x 2] integer array of 2D image coordinates.\n",
    "        :return: an [N x 3] array of normalized depth directions.\n",
    "        \"\"\"\n",
    "        _ = coords\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def center_crop(self) -> \"Camera\":\n",
    "        \"\"\"\n",
    "        Creates a new camera with the same intrinsics and direction as this one,\n",
    "        but with a center crop to a square of the smaller dimension.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def resize_image(self, width: int, height: int) -> \"Camera\":\n",
    "        \"\"\"\n",
    "        Creates a new camera with the same intrinsics and direction as this one,\n",
    "        but with resized image dimensions.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def scale_scene(self, factor: float) -> \"Camera\":\n",
    "        \"\"\"\n",
    "        Creates a new camera with the same intrinsics and direction as this one,\n",
    "        but with the scene rescaled by the given factor.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProjectiveCamera(Camera):\n",
    "    \"\"\"\n",
    "    A Camera implementation for a standard pinhole camera.\n",
    "\n",
    "    The camera rays shoot away from the origin in the z direction, with the x\n",
    "    and y directions corresponding to the positive horizontal and vertical axes\n",
    "    in image space.\n",
    "    \"\"\"\n",
    "\n",
    "    origin: np.ndarray\n",
    "    x: np.ndarray\n",
    "    y: np.ndarray\n",
    "    z: np.ndarray\n",
    "    width: int\n",
    "    height: int\n",
    "    x_fov: float\n",
    "    y_fov: float\n",
    "\n",
    "    def image_coords(self) -> np.ndarray:\n",
    "        ind = np.arange(self.width * self.height)\n",
    "        coords = np.stack([ind % self.width, ind // self.width], axis=1).astype(np.float32)\n",
    "        return coords\n",
    "\n",
    "    def camera_rays(self, coords: np.ndarray) -> np.ndarray:\n",
    "        fracs = (coords / (np.array([self.width, self.height], dtype=np.float32) - 1)) * 2 - 1\n",
    "        fracs = fracs * np.tan(np.array([self.x_fov, self.y_fov]) / 2)\n",
    "        directions = self.z + self.x * fracs[:, :1] + self.y * fracs[:, 1:]\n",
    "        directions = directions / np.linalg.norm(directions, axis=-1, keepdims=True)\n",
    "        return np.stack([np.broadcast_to(self.origin, directions.shape), directions], axis=1)\n",
    "\n",
    "    def depth_directions(self, coords: np.ndarray) -> np.ndarray:\n",
    "        return np.tile((self.z / np.linalg.norm(self.z))[None], [len(coords), 1])\n",
    "\n",
    "    def resize_image(self, width: int, height: int) -> \"ProjectiveCamera\":\n",
    "        \"\"\"\n",
    "        Creates a new camera for the resized view assuming the aspect ratio does not change.\n",
    "        \"\"\"\n",
    "        assert width * self.height == height * self.width, \"The aspect ratio should not change.\"\n",
    "        return ProjectiveCamera(\n",
    "            origin=self.origin,\n",
    "            x=self.x,\n",
    "            y=self.y,\n",
    "            z=self.z,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            x_fov=self.x_fov,\n",
    "            y_fov=self.y_fov,\n",
    "        )\n",
    "\n",
    "    def center_crop(self) -> \"ProjectiveCamera\":\n",
    "        \"\"\"\n",
    "        Creates a new camera for the center-cropped view\n",
    "        \"\"\"\n",
    "        size = min(self.width, self.height)\n",
    "        fov = min(self.x_fov, self.y_fov)\n",
    "        return ProjectiveCamera(\n",
    "            origin=self.origin,\n",
    "            x=self.x,\n",
    "            y=self.y,\n",
    "            z=self.z,\n",
    "            width=size,\n",
    "            height=size,\n",
    "            x_fov=fov,\n",
    "            y_fov=fov,\n",
    "        )\n",
    "\n",
    "    def scale_scene(self, factor: float) -> \"ProjectiveCamera\":\n",
    "        \"\"\"\n",
    "        Creates a new camera with the same intrinsics and direction as this one,\n",
    "        but with the camera frame rescaled by the given factor.\n",
    "        \"\"\"\n",
    "        return ProjectiveCamera(\n",
    "            origin=self.origin * factor,\n",
    "            x=self.x,\n",
    "            y=self.y,\n",
    "            z=self.z,\n",
    "            width=self.width,\n",
    "            height=self.height,\n",
    "            x_fov=self.x_fov,\n",
    "            y_fov=self.y_fov,\n",
    "        )\n",
    "\n",
    "\n",
    "class ViewData(ABC):\n",
    "    \"\"\"\n",
    "    A collection of rendered camera views of a scene or object.\n",
    "\n",
    "    This is a generalization of a NeRF dataset, since NeRF datasets only encode\n",
    "    RGB or RGBA data, whereas this dataset supports arbitrary channels.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def num_views(self) -> int:\n",
    "        \"\"\"\n",
    "        The number of rendered views.\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def channel_names(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get all of the supported channels available for the views.\n",
    "\n",
    "        This can be arbitrary, but there are some standard names:\n",
    "        \"R\", \"G\", \"B\", \"A\" (alpha), and \"D\" (depth).\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_view(self, index: int, channels: List[str]) -> Tuple[Camera, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Load the given channels from the view at the given index.\n",
    "\n",
    "        :return: a tuple (camera_view, data), where data is a float array of\n",
    "                 shape [height x width x num_channels].\n",
    "        \"\"\"\n",
    "\n",
    "class Front3DBlenderViewData(ViewData):\n",
    "    \"\"\"\n",
    "    Interact with a dataset zipfile exported by view_data.py.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, render_path, depth_path, camera_path):\n",
    "        # self.zipfile = zipfile.ZipFile(f_obj, mode=\"r\")\n",
    "        # self.infos = []\n",
    "        # with self.zipfile.open(\"info.json\", \"r\") as f:\n",
    "        #     self.info = json.load(f)\n",
    "        # assert all(k in cam_info for k in [\"origin\", \"x\", \"y\", \"z\", \"x_fov\", \"y_fov\"])\n",
    "        self.render_path = render_path\n",
    "        self.depth_path = depth_path\n",
    "        self.camera_path = camera_path\n",
    "        camera = np.load(os.path.join(camera_path, \"boxes.npz\"), allow_pickle=True)\n",
    "        self.build_cam_info(camera)\n",
    "        # self.channels = list(self.info.get(\"channels\", \"RGBAD\"))\n",
    "        self.channels = list(\"RGBAD\")\n",
    "        assert set(\"RGBA\").issubset(\n",
    "            set(self.channels)\n",
    "        ), \"The blender output should at least have RGBA images.\"\n",
    "        # names = set(x.filename for x in self.zipfile.infolist())\n",
    "        # for i in itertools.count():\n",
    "        #     name = f\"{i:05}.json\"\n",
    "        #     if name not in names:\n",
    "        #         break\n",
    "        #     with self.zipfile.open(name, \"r\") as f:\n",
    "        #         self.infos.append(json.load(f))\n",
    "\n",
    "    @property\n",
    "    def num_views(self) -> int:\n",
    "        return len(self.infos)\n",
    "\n",
    "    @property\n",
    "    def channel_names(self) -> List[str]:\n",
    "        return list(self.channels)\n",
    "    \n",
    "    def build_cam_info(self, camera):\n",
    "        # camera[\"camera_coords\"][1], camera[\"camera_coords\"][2] = camera[\"camera_coords\"][2], camera[\"camera_coords\"][1]\n",
    "        # print(\"[Initial]\", \"camera_coords\", camera[\"camera_coords\"][0], \"target_coords\", camera[\"target_coords\"][0])\n",
    "        camera_coords = camera[\"camera_coords\"]# [:, [0, 2, 1]]\n",
    "        target_coords = camera[\"target_coords\"]# [:, [0, 2, 1]]\n",
    "        floor_plan_centroid = camera[\"floor_plan_centroid\"]# [[0, 2, 1]]\n",
    "        # camera[\"camera_coords\"] += camera[\"floor_plan_centroid\"]\n",
    "        # camera[\"target_coords\"] += camera[\"floor_plan_centroid\"]\n",
    "        print(\"[Before]\", \"camera_coords\", camera_coords[0], \"target_coords\", target_coords[0])\n",
    "        camera_coords = camera_coords + floor_plan_centroid # x coord 0.0, z coord -0.5 set\n",
    "        target_coords = target_coords + floor_plan_centroid # x coord 0.0, z coord -0.5 set\n",
    "        print(\"Floor Centroid:\", floor_plan_centroid)\n",
    "        print(\"[After]\", \"camera_coords\", camera_coords[0], \"target_coords\", target_coords[0])\n",
    "\n",
    "        forward_vec = target_coords - camera_coords\n",
    "        # forward_vec = np.array([0, 0, 0]) - camera[\"camera_coords\"]\n",
    "        # right_vec, up_vec, forward_vec = rotation_from_forward_vec(forward_vec)\n",
    "        rotation_matrix = rotation_from_forward_vec(forward_vec)\n",
    "        print(rotation_matrix.shape) # (40, 3, 3)\n",
    "        # transformation_matrix = build_transformation_matrix(camera_coords, rotation_matrix)\n",
    "        # a = rotation_matrix[:, :, [0, 1, 2]]\n",
    "        # print(len(a), a)\n",
    "        right_vec, up_vec, forward_vec = (\n",
    "            rotation_matrix[..., 0], rotation_matrix[..., 1], rotation_matrix[..., 2]\n",
    "        ) # rotation_matrix[:, :, [0, 1, 2]]\n",
    "        # right_vec, up_vec, forward_vec = local_to_global(np.stack([right_vec, up_vec, forward_vec], axis=1), transformation_matrix)\n",
    "        self.infos = []\n",
    "        # NOTE: Should we use [0, 0, 0] as origin?\n",
    "        # NOTE: Is right/up/forward vector correct?\n",
    "        # NOTE: Why fov value is too large?\n",
    "        for i in range(len(rotation_matrix)):\n",
    "            self.infos.append(\n",
    "                {\n",
    "                    \"origin\": camera_coords[i], # Camera origin\n",
    "                    \"x\": right_vec[i], # right\n",
    "                    \"y\": up_vec[i], # up\n",
    "                    \"z\": forward_vec[i], # forward\n",
    "                    \"x_fov\": 70, # 70\n",
    "                    \"y_fov\": 70, # 70\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def load_view(self, index: int, channels: List[str]) -> Tuple[Camera, np.ndarray]:\n",
    "        for ch in channels:\n",
    "            if ch not in self.channel_names:\n",
    "                raise ValueError(f\"unsupported channel: {ch}\")\n",
    "\n",
    "        channel_map = {}\n",
    "        if any(x in channels for x in \"RGBA\"):\n",
    "            rgba = np.array(Image.open(os.path.join(self.render_path, f\"{str(index).zfill(4)}_colors.png\"))) / 255.0\n",
    "            channel_map.update(zip(\"RGBA\", rgba.transpose([2, 0, 1])))\n",
    "        # NOTE: Use \"max_depth\"?\n",
    "        if \"D\" in channels:\n",
    "            depth = np.array(Image.open(os.path.join(self.depth_path, f\"{str(index).zfill(4)}_depth.png\")))[:, :, 0]\n",
    "            inf_dist = depth == np.max(depth)\n",
    "            # inf_dist = depth == 255\n",
    "            channel_map[\"D\"] = np.where(\n",
    "                inf_dist, \n",
    "                np.inf, \n",
    "                (1) * (depth.astype(np.float32) / 255.0) # max_depth: scaling points\n",
    "            )\n",
    "\n",
    "        # The order of channels is user-specified.\n",
    "        # for k in channels:\n",
    "        #     print(k, channel_map[k].shape)\n",
    "        combined = np.stack([channel_map[k] for k in channels], axis=-1)\n",
    "        # print(combined.shape)\n",
    "        h, w, _ = combined.shape\n",
    "        return self.camera(index, w, h), combined\n",
    "            \n",
    "\n",
    "    def camera(self, index: int, width: int, height: int) -> ProjectiveCamera:\n",
    "        info = self.infos[index]\n",
    "        return ProjectiveCamera(\n",
    "            origin=np.array(info[\"origin\"], dtype=np.float32),\n",
    "            x=np.array(info[\"x\"], dtype=np.float32), # right\n",
    "            y=np.array(info[\"y\"], dtype=np.float32), # up\n",
    "            z=np.array(info[\"z\"], dtype=np.float32), # forward\n",
    "            width=width,\n",
    "            height=height,\n",
    "            x_fov=info[\"x_fov\"],\n",
    "            y_fov=info[\"y_fov\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import struct\n",
    "from contextlib import contextmanager\n",
    "from typing import BinaryIO, Iterator, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def write_ply(\n",
    "    raw_f: BinaryIO,\n",
    "    coords: np.ndarray,\n",
    "    rgb: Optional[np.ndarray] = None,\n",
    "    faces: Optional[np.ndarray] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Write a PLY file for a mesh or a point cloud.\n",
    "\n",
    "    :param coords: an [N x 3] array of floating point coordinates.\n",
    "    :param rgb: an [N x 3] array of vertex colors, in the range [0.0, 1.0].\n",
    "    :param faces: an [N x 3] array of triangles encoded as integer indices.\n",
    "    \"\"\"\n",
    "    with buffered_writer(raw_f) as f:\n",
    "        f.write(b\"ply\\n\")\n",
    "        f.write(b\"format binary_little_endian 1.0\\n\")\n",
    "        f.write(bytes(f\"element vertex {len(coords)}\\n\", \"ascii\"))\n",
    "        f.write(b\"property float x\\n\")\n",
    "        f.write(b\"property float y\\n\")\n",
    "        f.write(b\"property float z\\n\")\n",
    "        if rgb is not None:\n",
    "            f.write(b\"property uchar red\\n\")\n",
    "            f.write(b\"property uchar green\\n\")\n",
    "            f.write(b\"property uchar blue\\n\")\n",
    "        if faces is not None:\n",
    "            f.write(bytes(f\"element face {len(faces)}\\n\", \"ascii\"))\n",
    "            f.write(b\"property list uchar int vertex_index\\n\")\n",
    "        f.write(b\"end_header\\n\")\n",
    "\n",
    "        if rgb is not None:\n",
    "            rgb = (rgb * 255.499).round().astype(int)\n",
    "            vertices = [\n",
    "                (*coord, *rgb)\n",
    "                for coord, rgb in zip(\n",
    "                    coords.tolist(),\n",
    "                    rgb.tolist(),\n",
    "                )\n",
    "            ]\n",
    "            format = struct.Struct(\"<3f3B\")\n",
    "            for item in vertices:\n",
    "                f.write(format.pack(*item))\n",
    "        else:\n",
    "            format = struct.Struct(\"<3f\")\n",
    "            for vertex in coords.tolist():\n",
    "                f.write(format.pack(*vertex))\n",
    "\n",
    "        if faces is not None:\n",
    "            format = struct.Struct(\"<B3I\")\n",
    "            for tri in faces.tolist():\n",
    "                f.write(format.pack(len(tri), *tri))\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def buffered_writer(raw_f: BinaryIO) -> Iterator[io.BufferedIOBase]:\n",
    "    if isinstance(raw_f, io.BufferedIOBase):\n",
    "        yield raw_f\n",
    "    else:\n",
    "        f = io.BufferedWriter(raw_f)\n",
    "        yield f\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import BinaryIO, Dict, List, Optional, Union\n",
    "\n",
    "import blobfile as bf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/root/dev\")\n",
    "sys.path.append(\"/root/dev/ShapeNet_rendering/get_colored_pcs\")\n",
    "# from ply_util import write_ply\n",
    "\n",
    "COLORS = frozenset([\"R\", \"G\", \"B\", \"A\"])\n",
    "\n",
    "\n",
    "def preprocess(data, channel):\n",
    "    if channel in COLORS:\n",
    "        return np.round(data * 255.0)\n",
    "    return data\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PointCloud:\n",
    "    \"\"\"\n",
    "    An array of points sampled on a surface. Each point may have zero or more\n",
    "    channel attributes.\n",
    "\n",
    "    :param coords: an [N x 3] array of point coordinates.\n",
    "    :param channels: a dict mapping names to [N] arrays of channel values.\n",
    "    \"\"\"\n",
    "\n",
    "    coords: np.ndarray\n",
    "    channels: Dict[str, np.ndarray]\n",
    "\n",
    "    @classmethod\n",
    "    def from_rgbd(cls, vd: ViewData, num_views: Optional[int] = None, idx: int = None) -> \"PointCloud\":\n",
    "        \"\"\"\n",
    "        Construct a point cloud from the given view data.\n",
    "\n",
    "        The data must have a depth channel. All other channels will be stored\n",
    "        in the `channels` attribute of the result.\n",
    "\n",
    "        Pixels in the rendered views are not converted into points in the cloud\n",
    "        if they have infinite depth or less than 1.0 alpha.\n",
    "        \"\"\"\n",
    "        channel_names = vd.channel_names\n",
    "        if \"D\" not in channel_names:\n",
    "            raise ValueError(f\"view data must have depth channel\")\n",
    "        depth_index = channel_names.index(\"D\")\n",
    "\n",
    "        all_coords = []\n",
    "        all_channels = defaultdict(list)\n",
    "\n",
    "        if num_views is None:\n",
    "            num_views = vd.num_views\n",
    "        for i in range(num_views):\n",
    "            if idx is not None:\n",
    "                i = idx\n",
    "            camera, channel_values = vd.load_view(i, channel_names)\n",
    "            flat_values = channel_values.reshape([-1, len(channel_names)])\n",
    "\n",
    "            # Create an array of integer (x, y) image coordinates for Camera methods.\n",
    "            image_coords = camera.image_coords()\n",
    "            # return image_coords\n",
    "\n",
    "            # Select subset of pixels that have meaningful depth/color.\n",
    "            image_mask = np.isfinite(flat_values[:, depth_index])\n",
    "            # return image_mask\n",
    "            if \"A\" in channel_names:\n",
    "                image_mask_alpha = image_mask & (flat_values[:, channel_names.index(\"A\")] >= 1 - 1e-5)\n",
    "            # return image_mask_alpha\n",
    "            # Valid pixel coords & values\n",
    "            image_coords = image_coords[image_mask_alpha]\n",
    "            flat_values = flat_values[image_mask_alpha]\n",
    "\n",
    "            # Use the depth and camera information to compute the coordinates\n",
    "            # corresponding to every visible pixel.\n",
    "            camera_rays = camera.camera_rays(image_coords)\n",
    "            # return camera_rays\n",
    "            camera_origins = camera_rays[:, 0]\n",
    "            camera_directions = camera_rays[:, 1]\n",
    "            depth_dirs = camera.depth_directions(image_coords)\n",
    "            # return depth_dirs\n",
    "            ray_scales = flat_values[:, depth_index] / np.sum(\n",
    "                camera_directions * depth_dirs, axis=-1\n",
    "            )\n",
    "            # return ray_scales\n",
    "            coords = camera_origins + camera_directions * ray_scales[:, None]\n",
    "            # return coords\n",
    "\n",
    "            all_coords.append(coords)\n",
    "            for j, name in enumerate(channel_names):\n",
    "                if name != \"D\":\n",
    "                    all_channels[name].append(flat_values[:, j])\n",
    "\n",
    "        if len(all_coords) == 0:\n",
    "            return cls(coords=np.zeros([0, 3], dtype=np.float32), channels={})\n",
    "\n",
    "        return cls(\n",
    "            coords=np.concatenate(all_coords, axis=0),\n",
    "            channels={k: np.concatenate(v, axis=0) for k, v in all_channels.items()},\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, f: Union[str, BinaryIO]) -> \"PointCloud\":\n",
    "        \"\"\"\n",
    "        Load the point cloud from a .npz file.\n",
    "        \"\"\"\n",
    "        if isinstance(f, str):\n",
    "            with bf.BlobFile(f, \"rb\") as reader:\n",
    "                return cls.load(reader)\n",
    "        else:\n",
    "            obj = np.load(f)\n",
    "            keys = list(obj.keys())\n",
    "            return PointCloud(\n",
    "                coords=obj[\"coords\"],\n",
    "                channels={k: obj[k] for k in keys if k != \"coords\"},\n",
    "            )\n",
    "\n",
    "    def save(self, f: Union[str, BinaryIO]):\n",
    "        \"\"\"\n",
    "        Save the point cloud to a .npz file.\n",
    "        \"\"\"\n",
    "        if isinstance(f, str):\n",
    "            with bf.BlobFile(f, \"wb\") as writer:\n",
    "                self.save(writer)\n",
    "        else:\n",
    "            np.savez(f, coords=self.coords, **self.channels)\n",
    "\n",
    "    def write_ply(self, raw_f: BinaryIO):\n",
    "        write_ply(\n",
    "            raw_f,\n",
    "            coords=self.coords,\n",
    "            rgb=(\n",
    "                np.stack([self.channels[x] for x in \"RGB\"], axis=1)\n",
    "                if all(x in self.channels for x in \"RGB\")\n",
    "                else None\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def random_sample(self, num_points: int, **subsample_kwargs) -> \"PointCloud\":\n",
    "        \"\"\"\n",
    "        Sample a random subset of this PointCloud.\n",
    "\n",
    "        :param num_points: maximum number of points to sample.\n",
    "        :param subsample_kwargs: arguments to self.subsample().\n",
    "        :return: a reduced PointCloud, or self if num_points is not less than\n",
    "                 the current number of points.\n",
    "        \"\"\"\n",
    "        if len(self.coords) <= num_points:\n",
    "            return self\n",
    "        indices = np.random.choice(len(self.coords), size=(num_points,), replace=False)\n",
    "        return self.subsample(indices, **subsample_kwargs)\n",
    "\n",
    "    def farthest_point_sample(\n",
    "        self, num_points: int, init_idx: Optional[int] = None, **subsample_kwargs\n",
    "    ) -> \"PointCloud\":\n",
    "        \"\"\"\n",
    "        Sample a subset of the point cloud that is evenly distributed in space.\n",
    "\n",
    "        First, a random point is selected. Then each successive point is chosen\n",
    "        such that it is furthest from the currently selected points.\n",
    "\n",
    "        The time complexity of this operation is O(NM), where N is the original\n",
    "        number of points and M is the reduced number. Therefore, performance\n",
    "        can be improved by randomly subsampling points with random_sample()\n",
    "        before running farthest_point_sample().\n",
    "\n",
    "        :param num_points: maximum number of points to sample.\n",
    "        :param init_idx: if specified, the first point to sample.\n",
    "        :param subsample_kwargs: arguments to self.subsample().\n",
    "        :return: a reduced PointCloud, or self if num_points is not less than\n",
    "                 the current number of points.\n",
    "        \"\"\"\n",
    "        if len(self.coords) <= num_points:\n",
    "            return self\n",
    "        init_idx = random.randrange(len(self.coords)) if init_idx is None else init_idx\n",
    "        indices = np.zeros([num_points], dtype=np.int64)\n",
    "        indices[0] = init_idx\n",
    "        sq_norms = np.sum(self.coords**2, axis=-1)\n",
    "\n",
    "        def compute_dists(idx: int):\n",
    "            # Utilize equality: ||A-B||^2 = ||A||^2 + ||B||^2 - 2*(A @ B).\n",
    "            return sq_norms + sq_norms[idx] - 2 * (self.coords @ self.coords[idx])\n",
    "\n",
    "        cur_dists = compute_dists(init_idx)\n",
    "        for i in range(1, num_points):\n",
    "            idx = np.argmax(cur_dists)\n",
    "            indices[i] = idx\n",
    "\n",
    "            # Without this line, we may duplicate an index more than once if\n",
    "            # there are duplicate points, due to rounding errors.\n",
    "            cur_dists[idx] = -1\n",
    "\n",
    "            cur_dists = np.minimum(cur_dists, compute_dists(idx))\n",
    "\n",
    "        return self.subsample(indices, **subsample_kwargs)\n",
    "\n",
    "    def subsample(self, indices: np.ndarray, average_neighbors: bool = False) -> \"PointCloud\":\n",
    "        if not average_neighbors:\n",
    "            return PointCloud(\n",
    "                coords=self.coords[indices],\n",
    "                channels={k: v[indices] for k, v in self.channels.items()},\n",
    "            )\n",
    "\n",
    "        new_coords = self.coords[indices]\n",
    "        neighbor_indices = PointCloud(coords=new_coords, channels={}).nearest_points(self.coords)\n",
    "\n",
    "        # Make sure every point points to itself, which might not\n",
    "        # be the case if points are duplicated or there is rounding\n",
    "        # error.\n",
    "        neighbor_indices[indices] = np.arange(len(indices))\n",
    "\n",
    "        new_channels = {}\n",
    "        for k, v in self.channels.items():\n",
    "            v_sum = np.zeros_like(v[: len(indices)])\n",
    "            v_count = np.zeros_like(v[: len(indices)])\n",
    "            np.add.at(v_sum, neighbor_indices, v)\n",
    "            np.add.at(v_count, neighbor_indices, 1)\n",
    "            new_channels[k] = v_sum / v_count\n",
    "        return PointCloud(coords=new_coords, channels=new_channels)\n",
    "\n",
    "    def select_channels(self, channel_names: List[str]) -> np.ndarray:\n",
    "        data = np.stack([preprocess(self.channels[name], name) for name in channel_names], axis=-1)\n",
    "        return data\n",
    "\n",
    "    def nearest_points(self, points: np.ndarray, batch_size: int = 16384) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        For each point in another set of points, compute the point in this\n",
    "        pointcloud which is closest.\n",
    "\n",
    "        :param points: an [N x 3] array of points.\n",
    "        :param batch_size: the number of neighbor distances to compute at once.\n",
    "                           Smaller values save memory, while larger values may\n",
    "                           make the computation faster.\n",
    "        :return: an [N] array of indices into self.coords.\n",
    "        \"\"\"\n",
    "        norms = np.sum(self.coords**2, axis=-1)\n",
    "        all_indices = []\n",
    "        for i in range(0, len(points), batch_size):\n",
    "            batch = points[i : i + batch_size]\n",
    "            dists = norms + np.sum(batch**2, axis=-1)[:, None] - 2 * (batch @ self.coords.T)\n",
    "            all_indices.append(np.argmin(dists, axis=-1))\n",
    "        return np.concatenate(all_indices, axis=0)\n",
    "\n",
    "    def combine(self, other: \"PointCloud\") -> \"PointCloud\":\n",
    "        assert self.channels.keys() == other.channels.keys()\n",
    "        return PointCloud(\n",
    "            coords=np.concatenate([self.coords, other.coords], axis=0),\n",
    "            channels={\n",
    "                k: np.concatenate([v, other.channels[k]], axis=0) for k, v in self.channels.items()\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# origin = [0, 0, 0]\n",
    "# right_vec, up_vec, forward_vec = right_vec, up_vec, forward_vec\n",
    "# width = height = 256\n",
    "# x_fov, y_fov = x_fov, y_fov\n",
    "\n",
    "render_path = os.path.join(DATA_PATH, dir_names[\"img_depth_normal\"], EX_SCENE_ID)\n",
    "depth_path = os.path.join(DATA_PATH, dir_names[\"img_depth_normal\"], EX_SCENE_ID)\n",
    "camera_path = os.path.join(DATA_PATH, dir_names[\"cam\"], EX_SCENE_ID)\n",
    "\n",
    "vd = Front3DBlenderViewData(\n",
    "    render_path=render_path, \n",
    "    depth_path=depth_path,\n",
    "    camera_path=camera_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# # 3DGen info.json\n",
    "# import json\n",
    "\n",
    "# gen3d_camera_path = \"/root/node9/data/shape-generation/shapenetv1/rendered_shapenet_uniform_light/03001627/aaba865e99c23e7082db9fca4b68095/rendered_images\"\n",
    "\n",
    "# with open(os.path.join(gen3d_camera_path, \"info.json\"), \"r\") as f:\n",
    "#     info = json.load(f)\n",
    "# with open(os.path.join(gen3d_camera_path, \"00000.json\"), \"r\") as f:\n",
    "#     model_info = json.load(f)\n",
    "\n",
    "# print(\"[INFO]\")\n",
    "# for k, v in info.items():\n",
    "#     print(\"KEY:\", k)\n",
    "#     print(\"VALUE:\", v)\n",
    "# print(\"===============\")\n",
    "# print(\"[MODEL INFO]\")\n",
    "# for k, v in model_info.items():\n",
    "#     print(\"KEY:\", k)\n",
    "#     print(\"VALUE:\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# !pip install blobfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "num_images = 40\n",
    "pc = PointCloud.from_rgbd(vd, num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "coords = PointCloud.from_rgbd(vd, num_images)\n",
    "# print(image_coords)\n",
    "# print(image_mask, np.sum(image_mask)) # [False False False ...  True  True  True] 29717\n",
    "# print(image_mask_alpha, np.sum(image_mask_alpha)) # [False False False ...  True  True  True] 29717\n",
    "# print(camera_rays, camera_rays.shape) # (29717, 2, 3) -> Change to camera origin\n",
    "# print(depth_dirs, depth_dirs.shape) # [[ 0.80217993 -0.3533412   0.4813079 ], ...], (29717, 3)\n",
    "# print(ray_scales, ray_scales.shape) # (29717,)\n",
    "# print(np.sum(ray_scales == 0.)) # 318\n",
    "# print(coords, coords.shape) # point coords, (29717, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(\"coords\", pc.coords.shape)\n",
    "print(\"channels\")\n",
    "for k in pc.channels.keys():\n",
    "    print(\"KEY\", k, \"VALUE\", pc.channels[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "!pip install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# plt.ion()\n",
    "# print(\"Is interactive?\", plt.isinteractive())\n",
    "\n",
    "num_images = 1\n",
    "idx = 2\n",
    "pc = PointCloud.from_rgbd(vd, num_images, idx=idx)\n",
    "\n",
    "num_pts = 100_000\n",
    "sampled_pc = pc.random_sample(num_pts)\n",
    "print(\"sampled_pc.coords\", type(sampled_pc.coords), sampled_pc.coords.shape)\n",
    "cam_origin = vd.camera(idx, 256, 256).origin[None, :]\n",
    "print(\"cam_origin\", cam_origin, cam_origin.shape)\n",
    "# local to global\n",
    "# right, up, front = vd.camera(idx, 256, 256).x[None, :], vd.camera(idx, 256, 256).y[None, :], vd.camera(idx, 256, 256).z[None, :]\n",
    "# rotation_matrix = np.stack([right[0], up[0], front[0]], axis=-1)\n",
    "# transformation_matrix = build_transformation_matrix(cam_origin[0], rotation_matrix)\n",
    "# cam_origin = local_to_global(cam_origin, transformation_matrix)\n",
    "\n",
    "# for k in sampled_pc:\n",
    "#     print(k)\n",
    "# print(vars(sampled_pc))\n",
    "# print(sampled_pc.R.shape)\n",
    "sampled_pc_dict = vars(sampled_pc)\n",
    "print(type(sampled_pc_dict), sampled_pc_dict.keys())\n",
    "points = sampled_pc_dict[\"coords\"]\n",
    "# print(sampled_pc[\"channels\"][\"R\"].shape)\n",
    "colors = np.stack([\n",
    "    sampled_pc_dict[\"channels\"][\"R\"][:, None], \n",
    "    sampled_pc_dict[\"channels\"][\"G\"][:, None], \n",
    "    sampled_pc_dict[\"channels\"][\"B\"][:, None]\n",
    "], axis=-1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = Axes3D(fig)\n",
    "# ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(\n",
    "    points[:, 0], \n",
    "    points[:, 1], \n",
    "    points[:, 2], \n",
    "    s=1,\n",
    "    c=colors,\n",
    ")\n",
    "ax.scatter(\n",
    "    cam_origin[:, 0], \n",
    "    cam_origin[:, 1], \n",
    "    cam_origin[:, 2], \n",
    "    s=100,\n",
    "    c=\"green\",\n",
    ")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "plt.title(\"Colored Point Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# !pip install plotly\n",
    "# !pip install nbformat==4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# !pip list | grep nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# plt.ion()\n",
    "# print(\"Is interactive?\", plt.isinteractive())\n",
    "\n",
    "num_images = 1\n",
    "idx = 2\n",
    "pc = PointCloud.from_rgbd(vd, num_images, idx=idx)\n",
    "\n",
    "num_pts = 100_000\n",
    "sampled_pc = pc.random_sample(num_pts)\n",
    "print(\"sampled_pc.coords\", type(sampled_pc.coords), sampled_pc.coords.shape)\n",
    "cam_origin = vd.camera(idx, 256, 256).origin[None, :]\n",
    "print(\"cam_origin\", cam_origin, cam_origin.shape)\n",
    "# local to global\n",
    "# right, up, front = vd.camera(idx, 256, 256).x[None, :], vd.camera(idx, 256, 256).y[None, :], vd.camera(idx, 256, 256).z[None, :]\n",
    "# rotation_matrix = np.stack([right[0], up[0], front[0]], axis=-1)\n",
    "# transformation_matrix = build_transformation_matrix(cam_origin[0], rotation_matrix)\n",
    "# cam_origin = local_to_global(cam_origin, transformation_matrix)\n",
    "\n",
    "# for k in sampled_pc:\n",
    "#     print(k)\n",
    "# print(vars(sampled_pc))\n",
    "# print(sampled_pc.R.shape)\n",
    "sampled_pc_dict = vars(sampled_pc)\n",
    "print(type(sampled_pc_dict), sampled_pc_dict.keys())\n",
    "points = sampled_pc_dict[\"coords\"]\n",
    "# print(sampled_pc[\"channels\"][\"R\"].shape)\n",
    "colors = np.stack([\n",
    "    sampled_pc_dict[\"channels\"][\"R\"], \n",
    "    sampled_pc_dict[\"channels\"][\"G\"], \n",
    "    sampled_pc_dict[\"channels\"][\"B\"]\n",
    "], axis=-1)\n",
    "print(colors.shape)\n",
    "\n",
    "pointcloud = go.Scatter3d(\n",
    "    x=points[:, 0], \n",
    "    y=points[:, 1], \n",
    "    z=points[:, 2], \n",
    "    mode=\"markers\", \n",
    "    marker=dict(\n",
    "        size=2, \n",
    "        color=colors, \n",
    "        opacity=0.8\n",
    "    ), \n",
    "    name=\"PointCloud\"\n",
    ")\n",
    "camera_origin = go.Scatter3d(\n",
    "    x=cam_origin[:, 0],\n",
    "    y=cam_origin[:, 1],\n",
    "    z=cam_origin[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color=\"green\"\n",
    "    ),\n",
    "    name=\"Camera Origin\"\n",
    ")\n",
    "fig = go.Figure(data=[pointcloud, camera_origin])\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"X\",\n",
    "        yaxis_title=\"Y\",\n",
    "        zaxis_title=\"Z\"\n",
    "    ),\n",
    "    title=\"Interactive 3D Point Cloud\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "num_images = 2\n",
    "pc = PointCloud.from_rgbd(vd, num_images)\n",
    "\n",
    "num_pts = 100_000\n",
    "sampled_pc = pc.random_sample(num_pts)\n",
    "print(\"sampled_pc.coords\", type(sampled_pc.coords), sampled_pc.coords.shape)\n",
    "\n",
    "# for k in sampled_pc:\n",
    "#     print(k)\n",
    "# print(vars(sampled_pc))\n",
    "# print(sampled_pc.R.shape)\n",
    "sampled_pc_dict = vars(sampled_pc)\n",
    "print(type(sampled_pc_dict), sampled_pc_dict.keys())\n",
    "points = sampled_pc_dict[\"coords\"]\n",
    "# print(sampled_pc[\"channels\"][\"R\"].shape)\n",
    "colors = np.stack([\n",
    "    sampled_pc_dict[\"channels\"][\"R\"][:, None], \n",
    "    sampled_pc_dict[\"channels\"][\"G\"][:, None], \n",
    "    sampled_pc_dict[\"channels\"][\"B\"][:, None]\n",
    "], axis=-1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(\n",
    "    points[:, 0], \n",
    "    points[:, 1], \n",
    "    points[:, 2], \n",
    "    s=1,\n",
    "    c=colors,\n",
    ")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "plt.title(\"Colored Point Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "num_images = 1\n",
    "\n",
    "idx1 = 0\n",
    "pc1 = PointCloud.from_rgbd(vd, num_images, idx=idx1)\n",
    "num_pts = 100_000\n",
    "sampled_pc1 = pc1.random_sample(num_pts)\n",
    "cam1 = vd.camera(idx1, 256, 256)\n",
    "cam1_origin = cam1.origin[np.newaxis, :]\n",
    "print(\"Cam1:\", cam1_origin)\n",
    "x1, y1, z1 = cam1.x, cam1.y, cam1.z\n",
    "# print(\"sampled_pc.coords\", type(sampled_pc.coords), sampled_pc.coords.shape)\n",
    "\n",
    "idx2 = 2\n",
    "pc2 = PointCloud.from_rgbd(vd, num_images, idx=idx2)\n",
    "num_pts = 100_000\n",
    "sampled_pc2 = pc2.random_sample(num_pts)\n",
    "cam2 = vd.camera(idx2, 256, 256)\n",
    "cam2_origin = cam2.origin[np.newaxis, :]\n",
    "print(\"Cam2:\", cam2_origin)\n",
    "x2, y2, z2 = cam2.x, cam2.y, cam2.z\n",
    "\n",
    "# for k in sampled_pc:\n",
    "#     print(k)\n",
    "# print(vars(sampled_pc))\n",
    "# print(sampled_pc.R.shape)\n",
    "sampled_pc_dict1 = vars(sampled_pc1)\n",
    "sampled_pc_dict2 = vars(sampled_pc2)\n",
    "# print(type(sampled_pc_dict), sampled_pc_dict.keys())\n",
    "points1 = sampled_pc_dict1[\"coords\"]\n",
    "points2 = sampled_pc_dict2[\"coords\"]\n",
    "# print(sampled_pc[\"channels\"][\"R\"].shape)\n",
    "colors1 = np.stack([\n",
    "    sampled_pc_dict1[\"channels\"][\"R\"][:, None], \n",
    "    sampled_pc_dict1[\"channels\"][\"G\"][:, None], \n",
    "    sampled_pc_dict1[\"channels\"][\"B\"][:, None]\n",
    "], axis=-1)\n",
    "colors2 = np.stack([\n",
    "    sampled_pc_dict2[\"channels\"][\"R\"][:, None], \n",
    "    sampled_pc_dict2[\"channels\"][\"G\"][:, None], \n",
    "    sampled_pc_dict2[\"channels\"][\"B\"][:, None]\n",
    "], axis=-1)\n",
    "cams = np.concatenate([cam1_origin, cam2_origin], axis=0)\n",
    "print(cams)\n",
    "\n",
    "points1 = np.concatenate([points1, cam1_origin], axis=0)\n",
    "points2 = np.concatenate([points2, cam2_origin], axis=0)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(\n",
    "    points1[:, 0], \n",
    "    points1[:, 1], \n",
    "    points1[:, 2], \n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    "    c='blue',\n",
    ")\n",
    "ax.scatter(\n",
    "    points2[:, 0], \n",
    "    points2[:, 1], \n",
    "    points2[:, 2], \n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    "    c='red',\n",
    ")\n",
    "# ax.scatter(\n",
    "#     cams[:, 0], \n",
    "#     cams[:, 1], \n",
    "#     cams[:, 2], \n",
    "#     s=100,\n",
    "#     alpha=1.0, \n",
    "#     marker=f\"${idx1}$\",\n",
    "#     c=\"green\"\n",
    "# )\n",
    "ax.scatter(\n",
    "    cam1_origin[:, 0], \n",
    "    cam1_origin[:, 1], \n",
    "    cam1_origin[:, 2], \n",
    "    s=100,\n",
    "    alpha=1.0, \n",
    "    marker=f\"${idx1}$\",\n",
    "    c=\"green\"\n",
    ")\n",
    "ax.scatter(\n",
    "    cam2_origin[:, 0], \n",
    "    cam2_origin[:, 1], \n",
    "    cam2_origin[:, 2], \n",
    "    s=100,\n",
    "    alpha=1.0, \n",
    "    marker=f\"${idx2}$\",\n",
    "    c=\"green\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "plt.title(\"Colored Point Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "num_images = 1\n",
    "\n",
    "idx1 = 0\n",
    "pc1 = PointCloud.from_rgbd(vd, num_images, idx=idx1)\n",
    "num_pts = 100_000\n",
    "sampled_pc1 = pc1.random_sample(num_pts)\n",
    "cam1 = vd.camera(idx1, 256, 256)\n",
    "cam1_origin = cam1.origin[np.newaxis, :]\n",
    "print(\"Cam1:\", cam1_origin)\n",
    "x1, y1, z1 = cam1.x, cam1.y, cam1.z\n",
    "rotation_matrix1 = np.stack([x1, y1, z1], axis=-1)\n",
    "transformation_matrix1 = build_transformation_matrix(cam1_origin[0], rotation_matrix1)\n",
    "# print(\"sampled_pc.coords\", type(sampled_pc.coords), sampled_pc.coords.shape)\n",
    "\n",
    "idx2 = 2\n",
    "pc2 = PointCloud.from_rgbd(vd, num_images, idx=idx2)\n",
    "num_pts = 100_000\n",
    "sampled_pc2 = pc2.random_sample(num_pts)\n",
    "cam2 = vd.camera(idx2, 256, 256)\n",
    "cam2_origin = cam2.origin[np.newaxis, :]\n",
    "print(\"Cam2:\", cam2_origin)\n",
    "x2, y2, z2 = cam2.x, cam2.y, cam2.z\n",
    "rotation_matrix2 = np.stack([x2, y2, z2], axis=-1)\n",
    "transformation_matrix2 = build_transformation_matrix(cam2_origin[0], rotation_matrix2)\n",
    "\n",
    "# for k in sampled_pc:\n",
    "#     print(k)\n",
    "# print(vars(sampled_pc))\n",
    "# print(sampled_pc.R.shape)\n",
    "sampled_pc_dict1 = vars(sampled_pc1)\n",
    "sampled_pc_dict2 = vars(sampled_pc2)\n",
    "# print(type(sampled_pc_dict), sampled_pc_dict.keys())\n",
    "points1 = sampled_pc_dict1[\"coords\"]\n",
    "points2 = sampled_pc_dict2[\"coords\"]\n",
    "# local to global\n",
    "points1 = local_to_global(points1, transformation_matrix1)\n",
    "points2 = local_to_global(points2, transformation_matrix2)\n",
    "# print(sampled_pc[\"channels\"][\"R\"].shape)\n",
    "colors1 = np.stack([\n",
    "    sampled_pc_dict1[\"channels\"][\"R\"][:, None], \n",
    "    sampled_pc_dict1[\"channels\"][\"G\"][:, None], \n",
    "    sampled_pc_dict1[\"channels\"][\"B\"][:, None]\n",
    "], axis=-1)\n",
    "colors2 = np.stack([\n",
    "    sampled_pc_dict2[\"channels\"][\"R\"][:, None], \n",
    "    sampled_pc_dict2[\"channels\"][\"G\"][:, None], \n",
    "    sampled_pc_dict2[\"channels\"][\"B\"][:, None]\n",
    "], axis=-1)\n",
    "cams = np.concatenate([cam1_origin, cam2_origin], axis=0)\n",
    "print(cams)\n",
    "\n",
    "points1 = np.concatenate([points1, cam1_origin], axis=0)\n",
    "points2 = np.concatenate([points2, cam2_origin], axis=0)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(\n",
    "    points1[:, 0], \n",
    "    points1[:, 1], \n",
    "    points1[:, 2], \n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    "    c='blue',\n",
    ")\n",
    "ax.scatter(\n",
    "    points2[:, 0], \n",
    "    points2[:, 1], \n",
    "    points2[:, 2], \n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    "    c='red',\n",
    ")\n",
    "# ax.scatter(\n",
    "#     cams[:, 0], \n",
    "#     cams[:, 1], \n",
    "#     cams[:, 2], \n",
    "#     s=100,\n",
    "#     alpha=1.0, \n",
    "#     marker=f\"${idx1}$\",\n",
    "#     c=\"green\"\n",
    "# )\n",
    "ax.scatter(\n",
    "    cam1_origin[:, 0], \n",
    "    cam1_origin[:, 1], \n",
    "    cam1_origin[:, 2], \n",
    "    s=100,\n",
    "    alpha=1.0, \n",
    "    marker=f\"${idx1}$\",\n",
    "    c=\"green\"\n",
    ")\n",
    "ax.scatter(\n",
    "    cam2_origin[:, 0], \n",
    "    cam2_origin[:, 1], \n",
    "    cam2_origin[:, 2], \n",
    "    s=100,\n",
    "    alpha=1.0, \n",
    "    marker=f\"${idx2}$\",\n",
    "    c=\"green\"\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "plt.title(\"Colored Point Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Object data example\n",
    "import sys\n",
    "sys.path.append(\"/root/dev\")\n",
    "sys.path.append(\"/root/dev/ShapeNet_rendering/get_colored_pcs\")\n",
    "from ShapeNet_rendering.get_colored_pcs.view_data import BlenderViewData\n",
    "\n",
    "data_path = \"/root/data/shape-generation/shapenetv1/rendered_shapenet_uniform_light/03001627/aaba865e99c23e7082db9fca4b68095/rendered_images.zip\"\n",
    "vd_obj = BlenderViewData(data_path)\n",
    "\n",
    "# coords = PointCloud.from_rgbd(vd_obj, 1)\n",
    "# print(image_coords)\n",
    "# print(image_mask, np.sum(image_mask)) # [False False False ... False False False] 74790\n",
    "# print(image_mask_alpha, np.sum(image_mask_alpha)) # [False False False ... False False False] 73494\n",
    "# print(camera_rays, camera_rays.shape) # (73494, 2, 3)\n",
    "# print(depth_dirs, depth_dirs.shape) # [[-0.54642206  0.5171353  -0.6587822 ], ...] (73494, 3)\n",
    "# print(ray_scales, ray_scales.shape) # (73494,)\n",
    "# print(np.sum(ray_scales == 0.)) # 0\n",
    "# print(coords, coords.shape) # point coords, (73494, 3)\n",
    "\n",
    "\n",
    "pc = PointCloud.from_rgbd(vd_obj, 20)\n",
    "print(type(pc.coords), type(pc.channels))\n",
    "sampled_pc = pc.random_sample(100_000)\n",
    "sampled_pc_dict = vars(sampled_pc)\n",
    "print(type(sampled_pc_dict), sampled_pc_dict.keys())\n",
    "points = sampled_pc_dict[\"coords\"]\n",
    "# print(sampled_pc[\"channels\"][\"R\"].shape)\n",
    "colors = np.stack([\n",
    "    sampled_pc_dict[\"channels\"][\"R\"][:, None], \n",
    "    sampled_pc_dict[\"channels\"][\"G\"][:, None], \n",
    "    sampled_pc_dict[\"channels\"][\"B\"][:, None]\n",
    "], axis=-1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(\n",
    "    points[:, 0], \n",
    "    points[:, 1], \n",
    "    points[:, 2], \n",
    "    s=1,\n",
    "    c=colors,\n",
    ")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "plt.title(\"Colored Point Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dgs-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
